Day 2:

### Transformer - a type of machine learning model
https://www.youtube.com/watch?v=ZXiruGOCn9s&ab_channel=IBMTechnology
Transformers are a type of model architecture in artificial intelligence (AI) and natural language processing (NLP) [gives human like generated responses] that have significantly advanced the field. Introduced in the paper "Attention is All You Need" by Vaswani et al. in 2017, transformers have become the backbone of many state-of-the-art models in NLP and other domains.

### Key Concepts of Transformers:

1. **Self-Attention Mechanism:**
The self-attention mechanism allows the model to weigh the importance of different words in a sentence relative to each other, regardless of their positions. This helps in capturing long-range dependencies and relationships in the data.

2. **Positional Encoding:**
Since transformers do not have an inherent sense of word order (unlike recurrent neural networks), positional encodings are added to the input embeddings to provide information about the position of words in the sequence.

ps: Recurrent Neural Networks: a class of artificial neural networks designed for processing sequences of data. Unlike traditional feedforward neural networks, RNNs have connections that form directed cycles, which enable them to maintain a 'memory' of previous inputs. This makes them particularly suitable for tasks where context and sequential order are important.

4. **Encoder-Decoder Architecture:**
Transformers consist of an encoder and a decoder. The encoder processes the input sequence and generates an internal representation, while the decoder uses this representation to produce the output sequence. In some implementations, only the encoder is used (e.g., BERT), and in others, only the decoder (e.g., GPT-generative pretrained transformer).

### Applications of Transformers:
1. Language modelling (GPT)
2. Text Classification (BERT) (sentiment analysis, spam detection, and topic classification)
3. Machine translation (google translate)
4. Question answering (chatbots, virtual assistants)
5. Text summarization
6. Image processing (like in Vision Transformers (ViTs), which apply the transformer architecture to image patches)

### Advantages of transformers:
1. Parallelization: Unlike recurrent neural networks (RNNs), transformers allow for parallel processing of sequences, significantly speeding up training and inference.
2. Scalability: Transformers can handle very large datasets and are scalable to very large models (e.g., GPT-3 with 175 billion parameters).
3. Flexibility: The self-attention mechanism makes transformers highly flexible and effective at capturing complex dependencies in data.
___

### Took a glance of:
All you need is attention
How to train data-efficient LLMs
___

### Epoch:
In the context of artificial intelligence (AI) and machine learning (ML), an **epoch** refers to one complete cycle through the entire training dataset. During an epoch, the learning algorithm processes each sample in the training set exactly once. This term is crucial in understanding the training process of models, especially neural networks.

### Key Points about Epochs

1. **Full Dataset Pass**:
   - An epoch represents one full pass of the learning algorithm over the entire training dataset. If the training dataset contains 1,000 samples, one epoch means the model has seen all 1,000 samples once.

2. **Multiple Epochs**:
   - Typically, training a model involves multiple epochs (why? -> to improve model performance). The number of epochs is a hyperparameter that you set before training. Common choices can range from a few epochs to hundreds or even thousands, depending on the complexity of the model and the dataset size.

3. **Batch Processing**:
   - During an epoch, data is often processed in smaller chunks called batches or mini-batches rather than all at once. This is particularly important for large datasets that cannot fit into memory all at once. Each batch is used to update the model's parameters (w and b, given f(x) = wx + b for a linear regression of sorts, and similarly when there are multiple parameters), a process called **batch gradient descent** (optimization algorithm used to train machine learning models, particularly neural networks. It involves calculating the gradient of the loss function with respect to the model's parameters using the entire training dataset. This gradient is then used to update the parameters in order to minimize the loss function).

4. **Learning Progress**:
   - The goal of multiple epochs is to iteratively improve the model's performance. With each epoch, the model is expected to get better at generalizing from the training data, reducing the training loss, and hopefully improving performance on unseen validation data.

### Example

Suppose you have a dataset with 10,000 samples and you set the batch size to 100. This means that each epoch will consist of 100 iterations (10,000 / 100 = 100). During each iteration, the model processes one batch of 100 samples, updates the parameters, and moves to the next batch until all samples have been processed.

### Convergence and Overfitting

1. **Convergence**:
   - The model is said to converge when additional epochs do not result in a significant decrease in the loss or improvement in performance (when it isn't contradictory). Proper convergence indicates that the model has effectively learned from the data.

2. **Overfitting**:
   - If the model is trained for too many epochs, it might start to overfit the training data, meaning it performs well on the training data but poorly on validation or test data (it has trained so much, that it can't think on its own for new inputs, maybe). Monitoring validation performance during training helps in deciding when to stop training to avoid overfitting.

### Practical Considerations

1. **Early Stopping**:
   - A technique where training is stopped early if the performance on the validation set starts to degrade, indicating the onset of overfitting.

2. **Learning Rate Schedules**:
   - Adjusting the learning rate during training can help in better convergence. Learning rate schedules or adaptive learning rates are used to fine-tune the learning process over epochs.

3. **Checkpointing**:
   - Saving the model's state at the end of each epoch or at regular intervals can be useful for resuming training if interrupted and for selecting the best model based on validation performance.

In summary, an epoch is a fundamental concept in the training of AI and ML models, representing a complete iteration over the training data, crucial for understanding how models learn and improve over time.
___

### Tokens

In the context of AI and natural language processing (NLP), a **token** is a single unit of meaningful data. Tokenization is the process of breaking down text into these individual units, which can then be used for analysis or processing by machine learning models.

### Types of Tokens
1. **Word Tokens**:
   - Each token corresponds to a word in the text. For example, the sentence "Hello, world!" would be tokenized into ["Hello", ",", "world", "!"].
2. **Subword Tokens**:
   - Instead of whole words, text can be tokenized into smaller subword units. This is useful for handling rare words and out-of-vocabulary terms. For example, "unhappiness" might be tokenized into ["un", "happiness"] or ["un", "##happy", "##ness"].
3. **Character Tokens**:
   - Each token is a single character. For example, "Hello" would be tokenized into ["H", "e", "l", "l", "o"].

### Tokenization Techniques
1. **Whitespace Tokenization**:
   - Splits text based on whitespace. For example, "Hello world" becomes ["Hello", "world"].
2. **Punctuation-Based Tokenization**:
   - Considers punctuation marks as separate tokens. For example, "Hello, world!" becomes ["Hello", ",", "world", "!"].
3. **Subword Tokenization**:
   - Methods like Byte Pair Encoding (BPE) and WordPiece break down words into subword units. For example, BPE might split "lower" into ["low", "er"] and "lowest" into ["low", "est"].
4. **Regex-Based Tokenization**:
   - Uses regular expressions to define patterns for tokenization. This can handle more complex rules, such as splitting on specific punctuation patterns or handling contractions.

### Importance of Tokens in NLP

1. **Input for Models**:
   - Tokens are the basic input units for NLP models, such as recurrent neural networks (RNNs), transformers, and language models like BERT and GPT. These models process text at the token level.
   - 
2. **Text Representation**:
   - Tokens are converted into numerical representations (embeddings) that models can understand. These embeddings capture the semantic meaning of the tokens.

3. **Handling Variability**:
   - Subword and character tokenization techniques help in handling rare words, typos, and morphological variations, making models more robust.

### Practical Examples

1. **Sentiment Analysis**:
   - For a sentence like "I love this product!", tokenization might result in ["I", "love", "this", "product", "!"]. These tokens are then processed to determine the sentiment of the sentence.

2. **Machine Translation**:
   - In translating "Hello, world!" to French, the tokens ["Hello", ",", "world", "!"] are mapped to their corresponding French tokens ["Bonjour", ",", "monde", "!"].

3. **Text Generation**:
   - Language models generate text one token at a time. For example, given the prompt "Once upon a", the model generates tokens sequentially to complete the sentence.

### Tokenization in Modern NLP Models

Modern NLP models like BERT, GPT-3, and T5 use sophisticated tokenization techniques:

- **BERT** uses WordPiece tokenization, which breaks words into subwords based on their frequency in the training corpus.
- **GPT-3** uses byte-level Byte Pair Encoding (BPE), which operates at the byte level, allowing it to handle a wide variety of characters and languages.
 
In conclusion, tokens are fundamental units in NLP, crucial for converting raw text into a form that machine learning models can process. The choice of tokenization method can significantly impact the performance and robustness of NLP models. Understanding and applying appropriate tokenization techniques is essential for developing effective AI applications in language understanding and generation.

### Tokenizer
A tokenizer is a tool or algorithm used in natural language processing (NLP) to break down text into smaller units called tokens. Tokenizers are essential for preparing text data for various NLP tasks, such as text classification, translation, and generation.

### Tokenization Techniques
1. Byte Pair Encoding (BPE):
   BPE is a subword tokenization technique that iteratively merges the most frequent pair of bytes or characters, forming subwords. It helps in handling rare words and out-of-vocabulary terms.
2. WordPiece:
   Similar to BPE, WordPiece is used in models like BERT. It breaks words into subword units based on their frequency in the training corpus.
3. SentencePiece:
   A type of subword tokenizer that treats text as a sequence of Unicode characters, allowing it to handle different languages and scripts uniformly. It is used in models like T5.
___

### Compression Methods: Lossy vs. Lossless

Compression is the process of encoding information using fewer bits. It is generally categorized into two types: lossy and lossless.

#### Lossless Compression
Lossless compression algorithms reduce file size without losing any information. The original data can be perfectly reconstructed from the compressed data. This type of compression is essential for applications where data integrity is crucial, such as text documents, executable files, and some types of images (like PNG).

**Examples of Lossless Compression Algorithms**:
  - **DEFLATE**: Used in ZIP, GZIP, and PNG files.
  - **LZ77/LZ78**: Basis for DEFLATE and other algorithms like LZW.
  - **LZW (Lempel-Ziv-Welch)**: Used in GIF files.
  - **Huffman Coding**: Often used in combination with other algorithms.
  - **Brotli**: Used in web applications for better compression ratios than DEFLATE.

#### Lossy Compression

Lossy compression algorithms reduce file size by removing some information, which results in a loss of quality. The original data cannot be perfectly reconstructed from the compressed data. Lossy compression is typically used for multimedia data like images, audio, and video, where a loss of some quality is acceptable to achieve significant file size reduction.

**Examples of Lossy Compression Algorithms**:
  - **JPEG**: Commonly used for photographic images.
  - **MP3**: Widely used for audio compression.
  - **AAC (Advanced Audio Coding)**: Used in streaming audio formats.
  - **H.264/AVC**: Common video compression standard.
  - **WebP**: An image format developed by Google that can be both lossy and lossless.

### Compression in Practice

- **Text Files**:
  - Lossless compression methods are typically used. ZIP files with DEFLATE compression are common.
- **Image Files**:
  - Lossless: PNG, TIFF.
  - Lossy: JPEG, WebP.
- **Audio Files**:
  - Lossless: FLAC, ALAC (Apple Lossless Audio Codec).
  - Lossy: MP3, AAC.
- **Video Files**:
  - Lossless: Some proprietary formats and codecs.
  - Lossy: H.264, H.265 (HEVC), VP9.

### Choosing Compression Methods

- **When to Use Lossless Compression**:
  - When every bit of the original data must be preserved.
  - For text documents, software, and some types of images (technical drawings, logos).

- **When to Use Lossy Compression**:
  - When a reduction in quality is acceptable in exchange for a significant decrease in file size.
  - For multimedia content where perfect fidelity is not necessary (photographs, music, videos).
 
PS: also read about compression artifacts
___

Read about Diffusion models (not that clear but got an idea).
Tested the phi 3 model on huggingface. 
Tested out stabilityai/stable-diffusion-xl-base-1.0 since stable diffusion 1.5 had no model cards available. 
Found some models that couldn't be run on huggingface directly. So they had to be run locally. (does the training happen locally and what exactly is the difference?)
Referred to the following YouTube videos:
https://www.youtube.com/watch?v=Ay5K4tog5NQ&list=LL&index=3&pp=gAQBiAQB
https://www.youtube.com/watch?v=yTJxDzqo4fQ&list=LL&index=1&t=60s&pp=gAQBiAQB
Still didn't figure out how to create an isolated conda environment (and what it means, logically).

Some more doubts:
What is inference API (in huggingface) and what is pinging an API (how, what, why)?
